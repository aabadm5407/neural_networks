{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Recurrent Neural Networks.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr6OHWXHaT0a",
        "colab_type": "text"
      },
      "source": [
        "# Recap of Neural Networks\n",
        "\n",
        "## Concepts\n",
        "\n",
        "**Perceptron** The Perceptron takes in some input values and generates an output value - via calculating the weighted sum of input values then applying an activation function (nonlinear transformation) to it.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573599295359_Screenshot+2019-11-12+14.54.53.png)\n",
        "\n",
        "**Multilayer Perceptron (MLP)**\n",
        "A Multilayer Perceptron is a feedforward neural network with many fully-connected layers that uses nonlinear activation functions. An MLP is the most basic form of a deep neural net if it has more than 2 layers.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573599514820_Screenshot+2019-11-12+14.58.29.png)\n",
        "\n",
        "\n",
        "**Backpropagation**\n",
        "Backpropagation is an algorithm to help a neural network learn by calculating the gradients, or a feedforward computational graph. It does so by propagating the gradients backwards into the network via differentiating them using the chain rule.\n",
        "\n",
        "### Building Blocks of Neural Networks\n",
        "**Activation Function**\n",
        "Activation functions (sigmoid, tanh, ReLU) are non-linear functions that are applied to the weighted sum of the inputs of a neuron. They enable neural nets to go beyond linear functions and approximate any complex function.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Vivienne_Sze/publication/315667264/figure/download/fig3/AS:669951052496900@1536740186369/Various-forms-of-non-linear-activation-functions-Figure-adopted-from-Caffe-Tutorial.png)\n",
        "\n",
        "**Batch Size**\n",
        "Total number of training examples present in a single batch. Large batch sizes can be great because they can harness the power of GPUs to process more training instances per time. Small batch sizes' performance generalizes better and are less memory intensive.\n",
        "\n",
        "**Categorical Cross-Entropy Loss**\n",
        "The categorical cross-entropy loss aka the negative log likelihood is a popular loss function for classification tasks, that measures the similarity between two probability distributions – the true and predicted labels.\n",
        "\n",
        "**Dropout**\n",
        "Dropout is a fantastic regularization technique that gives you a massive performance boost (~2% for state-of-the-art models) for how simple the technique actually is. All dropout does is randomly turn off a percentage of neurons at each layer, at each training step. This makes the network more robust because it can’t rely on any particular set of input neurons for making predictions. The knowledge is distributed amongst the whole network. Around 2^n (where n is the number of neurons in the architecture) slightly-unique neural networks are generated during the training process, and ensembled together to make predictions.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573598735731_Screenshot+2019-11-12+14.45.29.png)\n",
        "\n",
        "**Early Stopping** Early Stopping lets you train a model with more hidden layers, hidden neurons and for more epochs than you need – then just stopping training when performance stops improving consecutively for n epochs. It saves the best performing model for you and prevents overfitting.\n",
        "\n",
        "**Epoch** Epochs: number of times your network sees your data. One epoch is when an entire dataset is passed both forward and backward through the neural network only once.\n",
        "\n",
        "**Gradient Descent** Gradient descent finds the minimum value of a function by using an iterative optimization algorithm for differentiable functions. We use it to find the lowest point in our loss function.\n",
        "\n",
        "**Hidden Layer** A hidden layer in neural network is a layer in between input layers and output layers, where neurons take in a set of weighted inputs and produce an output through an activation function.\n",
        "\n",
        "**Hyperparameter Tuning** Hyperparameter tuning is the process of searching the hyperparameter space to find the best hyperparameters values, through grid, random or bayesian search.\n",
        "\n",
        "**Learning Rate** Learning rate is a scalar used to update model parameters during gradient descent. It is the factor by which we multiply the gradients. Used to determine the amount by which the weights are updated during training.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_39292DB9CE2A9400103E176C2ABC438C6A626910E9DBB0D6FBE28EE673C7492C_1565307718429_image.png)\n",
        "\n",
        "**Momentum**\n",
        "Gradient Descent takes tiny, consistent steps towards the local minima and when the gradients are tiny it can take a lot of time to converge. Momentum takes into account the previous gradients & accelerates small but consistent gradients. It accelerates convergence by pushing over valleys faster & avoiding local minima.\n",
        "\n",
        "**One Hot Encoding** Many machine learning algorithms cannot operate on label data directly. They need all input variables and output variables to be numeric. In general, this is mostly a constraint of the efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves. This means that categorical data must be converted to a numerical form. This is called one hot encoding.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573598356099_Screenshot+2019-11-12+14.39.07.png)\n",
        "\n",
        "### Basic Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdQQLlcqaT0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install wandb\n",
        "%pip install -qq wandb\n",
        "#import libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvUL1LBYaT0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras-perceptron/perceptron-normalize.py\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "\n",
        "# logging code\n",
        "run = wandb.init(entity=\"wandb\", project=\"bloomberg-class\")\n",
        "config = run.config\n",
        "config.concept = 'mlp'\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "img_width = X_train.shape[1]\n",
        "img_height = X_train.shape[2]\n",
        "\n",
        "# normalize data\n",
        "\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "labels = [str(i) for i in range(10)]\n",
        "\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(img_width, img_height)))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n",
        "          callbacks=[wandb.keras.WandbCallback(data_type=\"image\", labels=labels, save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-vcACHcaT1K",
        "colab_type": "text"
      },
      "source": [
        "# LSTMs\n",
        "\n",
        "## Concepts\n",
        "\n",
        "**Attention Mechanism**\n",
        "Attention Mechanisms are inspired by human visual attention, the ability to focus on specific parts of an image. Attention mechanisms can be incorporated in both Language Processing and Image Recognition architectures to help the network learn what to “focus” on when making predictions.\n",
        "\n",
        "**Bag of Words**\n",
        "Bag of words is a method of feature engineering for text. In the resulting BoW feature vector, each dimension represents whether a specific token is present in the corpus. BoW ignores the order of words.\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573602945512_Screenshot+2019-11-12+15.55.17.png)\n",
        "\n",
        "**Embedding**\n",
        "Embeddings map inputs like words or sentences to vectors of numbers.\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573602983980_Screenshot+2019-11-12+15.56.20.png)\n",
        "\n",
        "**GloVe**\n",
        "GloVe is an unsupervised algorithm to convert words into embeddings trained on co-occurrence statistics.\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573603065273_Screenshot+2019-11-12+15.57.14.png)\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573603065257_Screenshot+2019-11-12+15.57.32.png)\n",
        "\n",
        "**word2vec**\n",
        "word2vec is an algorithm that also learns word embeddings by trying to predict the context of words in a document. word2vec vectors have mathematical properties and can be added and subtracted. e.g. `vector('queen') = vector('king') - vector('man') + vector('woman')`.\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573602983972_Screenshot+2019-11-12+15.56.11.png)\n",
        "\n",
        "### Create Character Encodings\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573602895710_Screenshot+2019-11-12+15.54.42.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7s3VweZePzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility Functions\n",
        "import os\n",
        "import subprocess\n",
        "import wandb\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Get the imdb dataset\n",
        "if not os.path.exists(\"aclImdb_v1.tar.gz\"):\n",
        "    print(\"Downloading imdb dataset...\")\n",
        "    subprocess.check_output(\n",
        "        \"curl -OL http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz && tar xvfz aclImdb_v1.tar.gz\", shell=True)\n",
        "\n",
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k: (v+3) for k, v in word_to_id.items()}\n",
        "id_to_word = {value: key for key, value in word_to_id.items()}\n",
        "id_to_word[0] = \"\"  # Padding\n",
        "id_to_word[1] = \"\"  # Start token\n",
        "id_to_word[2] = \"�\"  # Unknown\n",
        "id_to_word[3] = \"\"  # End token\n",
        "\n",
        "class TextLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, inp, out):\n",
        "        self.inp = inp\n",
        "        self.out = out\n",
        "\n",
        "    def on_epoch_end(self, logs, epoch):\n",
        "        out = self.model.predict(self.inp)\n",
        "        data = [[decode(self.inp[i]), o, self.out[i]]\n",
        "                for i, o in enumerate(out)]\n",
        "        wandb.log({\"text\": wandb.Table(rows=data)}, commit=False)\n",
        "\n",
        "def cosine_sim(v1,v2):\n",
        "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "    for i in range(len(v1)):\n",
        "        x = v1[i]; y = v2[i]\n",
        "        sumxx += x*x\n",
        "        sumyy += y*y\n",
        "        sumxy += x*y\n",
        "    return sumxy/math.sqrt(sumxx*sumyy)\n",
        "\n",
        "def decode(words):\n",
        "    return ' '.join(id_to_word[id] for id in words if id > 0)\n",
        "\n",
        "if not os.path.exists(\"glove.6B.50d.txt\"):\n",
        "    print(\"Downloading glove embeddings...\")\n",
        "    subprocess.check_output(\n",
        "        \"curl -OL https://storage.googleapis.com/wandb/glove.6B.50d.txt\", shell=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxNd4zkd724D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect data\n",
        "vocab_size = 10000\n",
        "embedding_dims = 50\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "print(X_train[0])\n",
        "print(decode(X_train[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ADtgRX5amr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect Glove embeddings\n",
        "embeddings_index = dict()\n",
        "print(\"Loading embeddings\")\n",
        "f = open('glove.6B.50d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "    \n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dims))\n",
        "for index in range(vocab_size):\n",
        "    word = id_to_word[index]\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "f.close()\n",
        "film = embeddings_index[\"film\"]\n",
        "movie = embeddings_index[\"movie\"]\n",
        "book = embeddings_index[\"book\"]\n",
        "car = embeddings_index[\"car\"]\n",
        "truck = embeddings_index[\"truck\"]\n",
        "plane = embeddings_index[\"plane\"]\n",
        "print(embeddings_index[\"plane\"])\n",
        "print(cosine_sim(truck, film))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GURlXiUxaT1N",
        "colab_type": "text"
      },
      "source": [
        "### Predicting Sentiment Data using LSTMs\n",
        "\n",
        "RNNs are an ML algorithm used to model sequential data by saving hidden states. It uses the same parameters and performs the same calculations at each step, with different inputs. At each time step, it calculates a new hidden state (“memory”) based on the current input and the previous hidden state and persists the information by using an internal loop in the network. RNNs are able to capture and learn the order of inputs they receive. RNNs are used with sequential data, like in natural language processing. RNNs can succumb to vanishing and exploding gradient problems.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573671695722_Screenshot+2019-11-13+11.01.23.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKyoo6f6aT1Q",
        "colab_type": "text"
      },
      "source": [
        "**Long Short-Term Memory Unit (LSTM)**\n",
        "LSTM units in RNNs help combat the vanishing gradient problem. by using neurons with a memory cell and three gates:\n",
        "- input – determines how much of information from the previous layer gets stored in the cell\n",
        "- output – determines how of the next layer gets to know about the state of the current cell\n",
        "- forget – determines what to forget about the current state of the memory cell\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573671832985_Screenshot+2019-11-13+11.03.50.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJv5BlSBaT1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# No Glove Embeddings\n",
        "# examples/lstm/imdb-classifier/imdb-lstm.py\n",
        "import wandb\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "wandb.init(entity=\"wandb\", project=\"bloomberg-class\")\n",
        "config = wandb.config\n",
        "config.concept = \"lstm-embedding\"\n",
        "config.vocab_size = 1000\n",
        "config.maxlen = 300\n",
        "config.batch_size = 32\n",
        "config.embedding_dims = 50\n",
        "config.filters = 250\n",
        "config.kernel_size = 3\n",
        "config.hidden_dims = 100\n",
        "config.epochs = 10\n",
        "\n",
        "# Load and tokenize input\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=config.vocab_size)\n",
        "\n",
        "# Ensure all input is the same size\n",
        "X_train = sequence.pad_sequences(\n",
        "    X_train, maxlen=config.maxlen)\n",
        "X_test = sequence.pad_sequences(\n",
        "    X_test, maxlen=config.maxlen)\n",
        "\n",
        "# overide LSTM & GRU\n",
        "if 'GPU' in str(device_lib.list_local_devices()):\n",
        "    print(\"Using CUDA for RNN layers\")\n",
        "    LSTM = tf.keras.layers.CuDNNLSTM\n",
        "    GRU = tf.keras.layers.CuDNNGRU\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(config.vocab_size,\n",
        "                                    config.embedding_dims,\n",
        "                                    input_length=config.maxlen))\n",
        "model.add(LSTM(100))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=config.batch_size,\n",
        "          epochs=config.epochs,\n",
        "          validation_data=(X_test, y_test), callbacks=[wandb.keras.WandbCallback(save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kTvvY_0aT1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# examples/lstm/imdb-classifier/imdb-embedding.py\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.layers import LSTM, GRU, CuDNNLSTM, CuDNNGRU\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import os\n",
        "\n",
        "# set parameters:\n",
        "wandb.init(entity=\"wandb\", project=\"bloomberg-class\")\n",
        "config = wandb.config\n",
        "config.concept = 'lstm-glove'\n",
        "config.vocab_size = 1000\n",
        "config.maxlen = 300\n",
        "config.batch_size = 64\n",
        "config.embedding_dims = 50\n",
        "config.filters = 250\n",
        "config.kernel_size = 3\n",
        "config.hidden_dims = 100\n",
        "config.epochs = 10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=config.vocab_size)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=config.maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=config.maxlen)\n",
        "\n",
        "embeddings_index = dict()\n",
        "\n",
        "f = open('glove.6B.50d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((config.vocab_size, config.embedding_dims))\n",
        "for index in range(config.vocab_size):\n",
        "    word = id_to_word[index]\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "\n",
        "# overide LSTM & GRU\n",
        "if 'GPU' in str(device_lib.list_local_devices()):\n",
        "    print(\"Using CUDA for RNN layers\")\n",
        "    LSTM = CuDNNLSTM\n",
        "    GRU = CuDNNGRU\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(config.vocab_size, config.embedding_dims, input_length=config.maxlen,\n",
        "                                    weights=[embedding_matrix], trainable=True))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(config.hidden_dims))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=config.batch_size,\n",
        "          epochs=config.epochs,\n",
        "          validation_data=(X_test, y_test), callbacks=[TextLogger(X_test[:20], y_test[:20]), wandb.keras.WandbCallback(save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-S8_P_KaT1S",
        "colab_type": "text"
      },
      "source": [
        "### Predicting Sentiment using BiDirecitonal LSTMs\n",
        "\n",
        "Bidirectional RNNs are composed of two RNNs flowing in different directions, stacked on top of each other. The forward RNN reads the input sequence from start to end, while the backward RNN reads it from end to start. We combine their states by appending their vectors and in doing so we're able to make predictions by using the context from before and after the words.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573672185132_Screenshot+2019-11-13+11.09.42.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7FL5mj7aT1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# examples/lstm/imdb-classifier/imdb-lstm.py\n",
        "import wandb\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# set parameters:\n",
        "wandb.init(entity=\"wandb\", project=\"bloomberg-class\")\n",
        "config = wandb.config\n",
        "config.concept = 'lstm-bidir'\n",
        "config.vocab_size = 1000\n",
        "config.maxlen = 300\n",
        "config.batch_size = 32\n",
        "config.embedding_dims = 50\n",
        "config.filters = 250\n",
        "config.kernel_size = 3\n",
        "config.hidden_dims = 100\n",
        "config.epochs = 10\n",
        "\n",
        "# Load and tokenize input\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=config.vocab_size)\n",
        "\n",
        "# Ensure all input is the same size\n",
        "X_train = sequence.pad_sequences(\n",
        "    X_train, maxlen=config.maxlen)\n",
        "X_test = sequence.pad_sequences(\n",
        "    X_test, maxlen=config.maxlen)\n",
        "\n",
        "# overide LSTM & GRU\n",
        "if 'GPU' in str(device_lib.list_local_devices()):\n",
        "    print(\"Using CUDA for RNN layers\")\n",
        "    LSTM = tf.keras.layers.CuDNNLSTM\n",
        "    GRU = tf.keras.layers.CuDNNGRU\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(config.vocab_size,\n",
        "                                    config.embedding_dims,\n",
        "                                    input_length=config.maxlen))\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(config.hidden_dims)))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=config.batch_size,\n",
        "          epochs=config.epochs,\n",
        "          validation_data=(X_test, y_test), callbacks=[TextLogger(X_test[:20], y_test[:20]),\n",
        "                                                       wandb.keras.WandbCallback(save_model=False)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhS74aNaT1U",
        "colab_type": "text"
      },
      "source": [
        "### Seq2Seq Translation\n",
        "\n",
        "seq2seq models are a combination of two RNNs – one serving as the encoder, the other as the decoder. They are great for machine translation.\n",
        "\n",
        "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1573672286628_Screenshot+2019-11-13+11.11.20.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-01ge8w0aT1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# examples/lstm/seq2seq/train.py\n",
        "# adapted from https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "wandb.init(entity=\"wandb\", project=\"bloomberg-class\")\n",
        "config = wandb.config\n",
        "config.concept = 'seq2seq'\n",
        "\n",
        "\n",
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one hot integer representation\n",
        "    + Decode the one hot integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One hot encode given string C.\n",
        "        # Arguments\n",
        "            num_rows: Number of rows in the returned one hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "# Parameters for the model and dataset.\n",
        "config.training_size = 10000\n",
        "config.digits = 3\n",
        "config.hidden_size = 64\n",
        "config.batch_size = 64\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "maxlen = config.digits + 1 + config.digits + 1 + config.digits\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+- '\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print('Generating data...')\n",
        "while len(questions) < config.training_size:\n",
        "    def f(): return int(''.join(np.random.choice(list('0123456789'))\n",
        "                                for i in range(np.random.randint(1, config.digits + 1))))\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    \n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (maxlen - len(q))\n",
        "    ans = str(a + b)\n",
        "\n",
        "    # Pad answer - Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (config.digits + 1 - len(ans))\n",
        "\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "\n",
        "\n",
        "def log_table(epoch, logs):\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    data = []\n",
        "    print()\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = model.predict_classes(rowx, verbose=0)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Q', q, end=' ')\n",
        "        print('T', correct, end=' ')\n",
        "        if correct == guess:\n",
        "            print('☑', end=' ')\n",
        "        else:\n",
        "            print('☒', end=' ')\n",
        "        data.append([q, guess, correct])\n",
        "        print(guess)\n",
        "    wandb.log({\"examples\": wandb.Table(data=data)})\n",
        "\n",
        "\n",
        "log_table_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_table)\n",
        "\n",
        "print('Total addition questions:', len(questions))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(questions), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), config.digits + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, maxlen)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, config.digits + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "# digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(config.hidden_size,\n",
        "                               input_shape=(maxlen, len(chars))))\n",
        "model.add(tf.keras.layers.RepeatVector(config.digits + 1))\n",
        "model.add(tf.keras.layers.LSTM(config.hidden_size, return_sequences=True))\n",
        "model.add(tf.keras.layers.TimeDistributed(\n",
        "    tf.keras.layers.Dense(len(chars), activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=config.batch_size,\n",
        "          epochs=100,\n",
        "          validation_data=(x_val, y_val), callbacks=[wandb.keras.WandbCallback(), log_table_callback])\n",
        "\n",
        "\n",
        "# Show predictions against the validation dataset.\n",
        "for iteration in range(1, 10):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "\n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = model.predict_classes(rowx, verbose=0)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Q', q, end=' ')\n",
        "        print('T', correct, end=' ')\n",
        "        if correct == guess:\n",
        "            print('☑', end=' ')\n",
        "        else:\n",
        "            print('☒', end=' ')\n",
        "        print(guess)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}